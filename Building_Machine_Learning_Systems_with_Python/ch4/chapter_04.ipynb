{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building Machine Learning Systems with Python - Chapter 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code is supporting material for the book `Building Machine Learning Systems with Python` by [Willi Richert](https://www.linkedin.com/in/willirichert/) and [Luis Pedro Coelho](https://www.linkedin.com/in/luispedrocoelho/)  published by PACKT Publishing.\n",
    "\n",
    "It is made available under the MIT License.\n",
    "\n",
    "All code examples use Python in version..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3.7.3 (default, Mar 27 2019, 17:13:21) [MSC v.1915 64 bit (AMD64)]'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "sys.version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\gkanavenko\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Downloading the data\n",
    "In this chapter we will use the StackOverflow data from https://archive.org/download/stackexchange (while downloading, you have a couple hours time to contemplate whether now would be a good time to donate to the awesome archive.org :-) )\n",
    "\n",
    "Since it is updated on a regular basis, you might get slightly different numbers. In this chapter we use this version:\n",
    "```\n",
    "stackoverflow.com-Posts.7z                        08-Dec-2017 22:31     11.3G\n",
    "```\n",
    "After downloading it, you need to unzip it with [7-Zip](http://www.7-zip.de/download.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extracting and filtering it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "from collections import defaultdict\n",
    "from tqdm import tqdm_notebook as tqdm # we all love nice progress bars, don't we?\n",
    "try:\n",
    "    import ujson as json  # UltraJSON if available\n",
    "except:\n",
    "    print(\"You can also use the normal json module, but you get a XXX speedup if you use ujson instead.\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO change before merging to master\n",
    "#DATA_DIR = \"data\"  # put your posts-2012.xml into this directory\n",
    "DATA_DIR = r'data'\n",
    "\n",
    "YEAR = 2017 # will restrict the data to posts from this year\n",
    "\n",
    "fn_posts_all = os.path.join(DATA_DIR, \"Posts.xml\")\n",
    "fn_posts = os.path.join(DATA_DIR, \"posts-%i.xml\" % YEAR)\n",
    "fn_filtered = os.path.join(DATA_DIR, \"filtered-%i.tsv\" % YEAR)\n",
    "fn_filtered_meta = os.path.join(DATA_DIR, \"filtered-%i-meta.json\" % YEAR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample posts: C:\\Users\\gkanavenko\\mlda\\Building_Machine_Learning_Systems_with_Python\\ch4\\2017\\data\\sample.tsv\n",
      "sample meta: C:\\Users\\gkanavenko\\mlda\\Building_Machine_Learning_Systems_with_Python\\ch4\\2017\\data\\sample-meta.json\n"
     ]
    }
   ],
   "source": [
    "SAMPLE_DIR = '%i' % YEAR\n",
    "if not os.path.exists(SAMPLE_DIR):\n",
    "    os.mkdir(SAMPLE_DIR)\n",
    "\n",
    "if not os.path.exists(os.path.join(SAMPLE_DIR, 'data')):\n",
    "    os.mkdir(os.path.join(SAMPLE_DIR, 'data'))\n",
    "\n",
    "fn_sample = os.path.abspath(os.path.join(SAMPLE_DIR, 'data', \"sample.tsv\"))\n",
    "fn_sample_meta = os.path.abspath(os.path.join(SAMPLE_DIR, 'data', \"sample-meta.json\"))\n",
    "print(\"sample posts: %s\" % fn_sample)\n",
    "print(\"sample meta: %s\" % fn_sample_meta)\n",
    "\n",
    "CHART_DIR = os.path.join(SAMPLE_DIR, \"charts\")\n",
    "if not os.path.exists(CHART_DIR):\n",
    "    os.mkdir(CHART_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The 59GB in posts.xml is contain posts from 2008 to 2017. We will use only some posts from the last year, which provides enough fun for now. We could simply grep on the command line, but that would take quite a while."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting all posts from 2017 ...\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "[Errno 28] No space left on device",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-11-7a47bca6ff04>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     59\u001b[0m         \u001b[0myear\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_year\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mline\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     60\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0myear\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mYEAR\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 61\u001b[1;33m             \u001b[0mf_year\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mline\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     62\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0myear\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0myear\u001b[0m \u001b[1;33m>\u001b[0m \u001b[0mYEAR\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     63\u001b[0m             \u001b[1;32mbreak\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mOSError\u001b[0m: [Errno 28] No space left on device"
     ]
    }
   ],
   "source": [
    "year_match = re.compile(r'^\\s+<row [^>]*CreationDate=\"(\\d+)-')\n",
    "size = os.path.getsize(fn_posts_all)\n",
    "\n",
    "def get_year(line):\n",
    "    m = year_match.match(line)\n",
    "    if m is None:\n",
    "        return None\n",
    "    return int(m.group(1))\n",
    "\n",
    "print(\"Extracting all posts from %i ...\" % YEAR)\n",
    "with open(fn_posts_all, 'r', encoding='utf-8') as fa, open(fn_posts, 'w', encoding='utf-8') as f_year:\n",
    "    # first two lines are the xml header and <posts> tag\n",
    "    f_year.write('<?xml version=\"1.0\" encoding=\"utf-8\"?><posts>\\n')    \n",
    "    \n",
    "    right = size//2\n",
    "    delta = right\n",
    "    \n",
    "    # first find some post of YEAR\n",
    "    while True:\n",
    "        fa.seek(right)\n",
    "        fa.readline() # go to next newline\n",
    "        line = fa.readline()\n",
    "        \n",
    "        year = get_year(line)\n",
    "        \n",
    "        delta //= 2\n",
    "        assert delta > 0\n",
    "        \n",
    "        if year>YEAR:\n",
    "            right -= delta\n",
    "        elif year<YEAR:\n",
    "            right += delta\n",
    "        else:\n",
    "            break\n",
    "            \n",
    "    # then find where it starts\n",
    "    left = right//2\n",
    "    delta = left\n",
    "    while True:\n",
    "        fa.seek(left)\n",
    "        fa.readline() # go to next newline\n",
    "        line = fa.readline()\n",
    "        \n",
    "        year = get_year(line)\n",
    "        \n",
    "        delta //= 2\n",
    "        if delta == 0:\n",
    "            break\n",
    "        \n",
    "        if year<YEAR:\n",
    "            left += delta\n",
    "            \n",
    "        else:\n",
    "            left, right = left-delta, left\n",
    "    \n",
    "    # and write all posts of that year\n",
    "    while True:\n",
    "        line = fa.readline()\n",
    "        year = get_year(line)\n",
    "        if year == YEAR:\n",
    "            f_year.write(line)\n",
    "        elif year is None or year > YEAR:\n",
    "            break\n",
    "        \n",
    "    # and write the closing tag\n",
    "    f_year.write('</posts>')\n",
    "print('... done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from dateutil import parser as dateparser\n",
    "\n",
    "from operator import itemgetter\n",
    "from lxml import etree\n",
    "\n",
    "NUM_ROWS = 5113519 # counted by hand\n",
    "\n",
    "filtered_meta = {\n",
    "    'question': {}, # question -> [(answer Id, Score), ...]\n",
    "    'total': 0 # questions and answers finally written\n",
    "}\n",
    "\n",
    "# Regular expressions to find code snippets, links, and tags, which might help in \n",
    "# designing useful features\n",
    "code_match = re.compile('<pre>(.*?)</pre>', re.MULTILINE | re.DOTALL)\n",
    "link_match = re.compile('<a href=\"http://.*?\".*?>(.*?)</a>', re.MULTILINE | re.DOTALL)\n",
    "tag_match = re.compile('<[^>]*>', re.MULTILINE | re.DOTALL)\n",
    "whitespace_match = re.compile(r'\\s+', re.MULTILINE | re.DOTALL)\n",
    "\n",
    "def extract_features_from_body(s):\n",
    "    '''\n",
    "    This method creates features from the raw post. It already contains all \n",
    "    features that we will use throughout the chapter.\n",
    "    '''\n",
    "    num_code_lines = 0\n",
    "    link_count_in_code = 0\n",
    "    code_free_s = s\n",
    "\n",
    "    # remove source code and count how many lines\n",
    "    for match_str in code_match.findall(s):\n",
    "        num_code_lines += match_str.count('\\n')\n",
    "        code_free_s = code_match.sub(' ', code_free_s)\n",
    "\n",
    "        # sometimes source code contain links, which we don't want to count\n",
    "        link_count_in_code += len(link_match.findall(match_str))\n",
    "\n",
    "    links = link_match.findall(s)\n",
    "    link_count = len(links) - link_count_in_code\n",
    "\n",
    "    html_free_s = tag_match.sub(' ', code_free_s)\n",
    "    \n",
    "    text = html_free_s\n",
    "    for link in links:\n",
    "        if link.lower().startswith('http://'):\n",
    "            text = text.replace(link, ' ')\n",
    "\n",
    "    text = whitespace_match.sub(' ', text)\n",
    "    num_text_tokens = text.count(' ')\n",
    "\n",
    "    return text, num_text_tokens, num_code_lines, link_count\n",
    "\n",
    "num_questions = 0\n",
    "num_answers = 0\n",
    "\n",
    "def parsexml(fn):\n",
    "    global num_questions, num_answers\n",
    "\n",
    "    counter = 0\n",
    "\n",
    "    # iterparse() returns a tuple (event, element). Since we request only\n",
    "    # 'start' events, we pipe the result through an itemgetter that always returns\n",
    "    # the 2nd result.\n",
    "    it = map(itemgetter(1), etree.iterparse(fn, events=('start',)))\n",
    "    \n",
    "    # Get the <posts> element, in which we will parse the <row> elements. While doing so,\n",
    "    # we will need the root handle to clear memory\n",
    "    root = next(it)\n",
    "    \n",
    "    for counter, elem in enumerate(tqdm(it, total=NUM_ROWS)):\n",
    "        \n",
    "        if elem.tag != 'row':\n",
    "            continue\n",
    "            \n",
    "        Id = int(elem.get('Id'))\n",
    "        PostTypeId = int(elem.get('PostTypeId'))\n",
    "        Score = int(elem.get('Score'))\n",
    "\n",
    "        if PostTypeId == 1:\n",
    "            num_questions += 1            \n",
    "            ParentId = -1\n",
    "            filtered_meta['question'][Id] = []\n",
    "            \n",
    "        elif PostTypeId == 2:\n",
    "            num_answers += 1\n",
    "            ParentId = int(elem.get('ParentId'))\n",
    "            if not ParentId in filtered_meta['question']:\n",
    "                # question is not from the same year so we have already dropped it\n",
    "                continue\n",
    "\n",
    "            filtered_meta['question'][ParentId].append((Id, Score))\n",
    "\n",
    "        else:\n",
    "            continue\n",
    "\n",
    "        Text, NumTextTokens, NumCodeLines, LinkCount = extract_features_from_body(elem.get('Body'))\n",
    "\n",
    "        # We have to tell lxml that this element is not used anymore. Otherwise, memory will blow up.\n",
    "        # See https://www.ibm.com/developerworks/xml/library/x-hiperfparse for more information.\n",
    "        elem.clear()\n",
    "        while elem.getprevious() is not None:\n",
    "            del elem.getparent()[0]\n",
    "            \n",
    "        values = (Id, ParentId, Score, NumTextTokens, NumCodeLines, LinkCount, Text)\n",
    "\n",
    "        yield values\n",
    "\n",
    "    print(\"Found %i posts\" % counter)\n",
    "\n",
    "if any(not os.path.exists(fn) for fn in [fn_filtered, fn_filtered_meta]):\n",
    "    total = 0\n",
    "    with open(fn_filtered, \"w\", encoding='utf-8') as f:\n",
    "        for values in parsexml(fn_posts):\n",
    "            line = \"\\t\".join(map(str, values))\n",
    "            f.write(line + \"\\n\")\n",
    "            total += 1\n",
    "    filtered_meta['total'] = total\n",
    "                \n",
    "    with open(fn_filtered_meta, \"w\") as f:\n",
    "        json.dump(filtered_meta, f)\n",
    "    \n",
    "    print(\"#qestions: %i\" % num_questions)\n",
    "    print(\"#answers: %i\" % num_answers)\n",
    "    \n",
    "else:\n",
    "    print(\"Skipping the conversion step, loading data from %s ...\" % fn_filtered_meta)\n",
    "    filtered_meta = json.load(open(fn_filtered_meta, \"r\"))\n",
    "    print(\"... done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to select the answers that we want to keep per question. We do this in two stages:\n",
    " * Stage 1: Chosing questions that have a positive and negative answer and then chosing the most positive and negative.\n",
    " * Stage 2: Write out the features for those answers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_QUESTION_SAMPLE = 10000\n",
    "\n",
    "posts_to_keep = set()\n",
    "found_questions = 0\n",
    "\n",
    "question = filtered_meta['question']\n",
    "\n",
    "# Sorting the questions before iterating over them is only done for\n",
    "# reproducability.\n",
    "for ParentId, posts in tqdm(sorted(question.items()), desc=\"Stage 1:\"):\n",
    "    assert ParentId != -1\n",
    "\n",
    "    if len(posts) < 2:\n",
    "        continue\n",
    "\n",
    "    neg_score_ids = []\n",
    "    pos_score_ids = []\n",
    "    \n",
    "    for Id, Score in posts:\n",
    "        if Score < 0:\n",
    "            neg_score_ids.append((Score, Id))\n",
    "        elif Score > 0:\n",
    "            pos_score_ids.append((Score, Id))   \n",
    "\n",
    "    if pos_score_ids and neg_score_ids:\n",
    "        posts_to_keep.add(int(ParentId))\n",
    "\n",
    "        posScore, posId = sorted(pos_score_ids)[-1]\n",
    "        posts_to_keep.add(posId)\n",
    "\n",
    "        negScore, negId = sorted(neg_score_ids)[0]\n",
    "        posts_to_keep.add(negId)\n",
    "\n",
    "        found_questions += 1\n",
    "\n",
    "    if found_questions >= NUM_QUESTION_SAMPLE:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "already_written = set()\n",
    "sample_meta = defaultdict(dict)\n",
    "\n",
    "total = 0\n",
    "kept = 0\n",
    "\n",
    "with open(fn_sample, \"w\", encoding='utf-8') as f:\n",
    "    for line in tqdm(open(fn_filtered, 'r', encoding='utf-8'), total=NUM_ROWS, desc=\"Stage 2:\"):\n",
    "        Id, ParentId, Score, NumTextTokens, NumCodeLines, LinkCount, Text = line.split(\"\\t\")\n",
    "\n",
    "        Text = Text.strip()\n",
    "\n",
    "        total += 1\n",
    "\n",
    "        Id = int(Id)\n",
    "        if Id in posts_to_keep:\n",
    "            if Id in already_written:\n",
    "                print(Id, \"is already written\")\n",
    "                continue\n",
    "\n",
    "            # setting meta info\n",
    "            post = sample_meta[Id]\n",
    "            post['ParentId'] = int(ParentId)\n",
    "            post['Score'] = int(Score)\n",
    "            post['NumTextTokens'] = int(NumTextTokens)\n",
    "            post['NumCodeLines'] = int(NumCodeLines)\n",
    "            post['LinkCount'] = int(LinkCount)\n",
    "            post['idx'] = kept  # index into the TSV file\n",
    "\n",
    "            if int(ParentId) == -1:\n",
    "                q = sample_meta[Id]\n",
    "\n",
    "                if not 'Answers' in q:\n",
    "                    q['Answers'] = []\n",
    "\n",
    "            else:\n",
    "                q = sample_meta[int(ParentId)]\n",
    "\n",
    "                if 'Answers' not in q:\n",
    "                    q['Answers'] = [Id]\n",
    "                else:\n",
    "                    q['Answers'].append(Id)\n",
    "\n",
    "            f.writelines(\"%s\\t%s\\n\" % (Id, Text))\n",
    "            kept += 1\n",
    "\n",
    "with open(fn_sample_meta, \"w\") as fm:\n",
    "    json.dump(sample_meta, fm)\n",
    "\n",
    "print(\"read:\", total)\n",
    "print(\"kept:\", kept)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_meta(fn):\n",
    "    meta = json.load(open(fn, \"r\"))\n",
    "    \n",
    "    # JSON only allows string keys, changing that to int\n",
    "    for key in list(meta.keys()):\n",
    "        meta[int(key)] = meta[key]\n",
    "        del meta[key]\n",
    "\n",
    "    return meta\n",
    "\n",
    "meta = load_meta(fn_sample_meta)\n",
    "\n",
    "def save_png(name):\n",
    "    fn = 'B09124_04_%s.png'%name # please ignore, it just helps our publisher :-)\n",
    "    plt.savefig(os.path.join(CHART_DIR, fn), bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading the features and labeling them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_answers = sorted([a for a, v in meta.items() if v['ParentId'] != -1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An answer is labeled as positive if it has a score greater than zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = np.asarray([meta[aid]['Score'] > 0 for aid in all_answers])\n",
    "print(np.unique(Y, return_counts=True))\n",
    "# We will need a couple iterations on X further down..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating our first classifier: kNN using only LinkCount as a feature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So how good is `LinkCount`? Let's look at its histogram."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "X = np.asarray([[meta[aid]['LinkCount']] for aid in all_answers])\n",
    "\n",
    "plt.figure(figsize=(5,4), dpi=300) # width and height of the plot in inches\n",
    "\n",
    "plt.title('LinkCount')\n",
    "plt.xlabel('Value')\n",
    "plt.ylabel('Occurrence')\n",
    "\n",
    "n, bins, patches = plt.hist(X, normed=1, bins=range(max(X.ravel())-min(X.ravel())), alpha=0.75)\n",
    "\n",
    "plt.grid(True)\n",
    "save_png('01_feat_hist_LinkCount')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, so most posts don't contain a link at all, but let's try nevertheless..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training on LinkCount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "N_FOLDS = 10\n",
    "cv = KFold(n_splits=N_FOLDS, shuffle=True, random_state=0)\n",
    "\n",
    "scores = []\n",
    "for train, test in tqdm(cv.split(X, Y)):\n",
    "    clf = KNeighborsClassifier()\n",
    "    clf.fit(X[train], Y[train])\n",
    "    scores.append(clf.score(X[test], Y[test]))\n",
    "\n",
    "print(\"Mean(scores)=%.5f\\tStddev(scores)=%.5f\"%(np.mean(scores), np.std(scores))) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using more features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_feat_hist(data_name_list, filename=None):\n",
    "    if len(data_name_list) > 1:\n",
    "        assert filename is not None\n",
    "\n",
    "    num_rows = int(1 + (len(data_name_list) - 1) / 2)\n",
    "    num_cols = int(1 if len(data_name_list) == 1 else 2)\n",
    "    plt.figure(figsize=(5 * num_cols, 4 * num_rows), dpi=300)\n",
    "\n",
    "    for i in range(num_rows):\n",
    "        for j in range(num_cols):\n",
    "            plt.subplot(num_rows, num_cols, 1 + i * num_cols + j)\n",
    "            x, name = data_name_list[i * num_cols + j]\n",
    "            plt.title(name)\n",
    "            plt.xlabel('Value')\n",
    "            plt.ylabel('Occurrence')\n",
    "            \n",
    "            max_val = max(x.ravel())\n",
    "            if max_val>1000:\n",
    "                bins = range(0, max_val, 100)\n",
    "            elif max_val>100:\n",
    "                bins = range(0, max_val, 10)\n",
    "            else:\n",
    "                bins = range(0, max_val)\n",
    "            \n",
    "            n, bins, patches = plt.hist(x, bins=bins, normed=1, alpha=0.75)\n",
    "\n",
    "            plt.grid(True)\n",
    "            plt.tight_layout()\n",
    "    \n",
    "    if not filename:\n",
    "        filename = \"feat_hist_%s\" % name.replace(\" \", \"_\")\n",
    "\n",
    "    save_png(filename)\n",
    "\n",
    "\n",
    "plot_feat_hist([(np.asarray([[meta[aid]['NumCodeLines']] for aid in all_answers]), 'NumCodeLines'),\n",
    "                (np.asarray([[meta[aid]['NumTextTokens']] for aid in all_answers]), 'NumTextTokens')],\n",
    "              '02_feat_hist_CodeLines_TextTokens');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the features vary in their value ranges, we need to standardize them using `StandardScaler()` so that kNN does not bias towards features having larger value intervals. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "def get_features(aid, feature_names):\n",
    "    return tuple(meta[aid][fn] for fn in feature_names)\n",
    "\n",
    "X = np.asarray([get_features(aid, ['LinkCount', 'NumCodeLines', 'NumTextTokens']) for aid in all_answers], float)\n",
    "\n",
    "scores = []\n",
    "for train, test in tqdm(cv.split(X, Y), total=N_FOLDS):\n",
    "    clf = make_pipeline(StandardScaler(), KNeighborsClassifier())\n",
    "    clf.fit(X[train], Y[train])\n",
    "    scores.append(clf.score(X[test], Y[test]))\n",
    "\n",
    "print(\"Mean(scores)=%.5f\\tStddev(scores)=%.5f\"%(np.mean(scores), np.std(scores))) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Designing more features\n",
    "Let's create some more text based features like average sentence and word length, how many words are CAPITALIZED or contain exclamation marks.\n",
    "\n",
    "We simply fetch the post texts, calculate the statistics and add them to the `meta` dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "def fetch_posts(fn):\n",
    "    for line in open(fn, 'r', encoding='utf-8'):\n",
    "        post_id, text = line.split('\\t')\n",
    "        yield int(post_id), text.strip()\n",
    "\n",
    "def add_sentence_features(m):\n",
    "    for pid, text in fetch_posts(fn_sample):\n",
    "        if not text:\n",
    "            for feat in ['AvgSentLen', 'AvgWordLen', 'NumAllCaps', 'NumExclams']:\n",
    "                m[pid][feat] = 0\n",
    "        else:\n",
    "            sent_lens = [len(nltk.word_tokenize(sent)) for sent in nltk.sent_tokenize(text)]\n",
    "            m[pid]['AvgSentLen'] = np.mean(sent_lens)\n",
    "            text_tokens = nltk.word_tokenize(text)\n",
    "            m[pid]['AvgWordLen'] = np.mean([len(w) for w in text_tokens])\n",
    "            m[pid]['NumAllCaps'] = np.sum([word.isupper() for word in text_tokens])\n",
    "            m[pid]['NumExclams'] = text.count('!')\n",
    "\n",
    "add_sentence_features(meta)\n",
    "\n",
    "plot_feat_hist([(np.asarray([[meta[aid][feat]] for aid in all_answers], dtype=int), feat) for feat in ['AvgSentLen', 'AvgWordLen', 'NumAllCaps', 'NumExclams']],\n",
    "              '03_feat_hist_AvgSentLen_AvgWordLen_NumAllCaps_NumExclams');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.asarray([get_features(aid, ['LinkCount', 'NumCodeLines', 'NumTextTokens', \n",
    "                                   'AvgSentLen', 'AvgWordLen', 'NumAllCaps', \n",
    "                                   'NumExclams']) for aid in all_answers], float)\n",
    "\n",
    "scores = []\n",
    "for train, test in tqdm(cv.split(X, Y), total=N_FOLDS):    \n",
    "    clf = make_pipeline(StandardScaler(), KNeighborsClassifier())\n",
    "    clf.fit(X[train], Y[train])\n",
    "    scores.append(clf.score(X[test], Y[test]))\n",
    "\n",
    "print(\"Mean(scores)=%.5f\\tStddev(scores)=%.5f\"%(np.mean(scores), np.std(scores))) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# High or low bias?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_curve, roc_curve, auc, classification_report\n",
    "\n",
    "def plot_bias_variance(data_sizes, train_errors, test_errors, name, title):\n",
    "    plt.figure(num=None, figsize=(6, 5), dpi=300)\n",
    "    plt.ylim([0.0, 1.0])\n",
    "    plt.xlabel('Data set size')\n",
    "    plt.ylabel('Error')\n",
    "    plt.title(\"Bias-Variance for '%s'\" % name)\n",
    "    plt.plot(\n",
    "        data_sizes, test_errors, \"--\", data_sizes, train_errors, \"b-\", lw=1)\n",
    "    plt.legend([\"test error\", \"train error\"], loc=\"upper right\")\n",
    "    plt.grid(True)\n",
    "\n",
    "def plot_pr(auc_score, name, precision, recall, label=None):\n",
    "    plt.figure(num=None, figsize=(6, 5), dpi=300)\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.0])\n",
    "    plt.xlabel('Recall')\n",
    "    plt.ylabel('Precision')\n",
    "    plt.title('P/R (AUC=%0.2f) / %s' % (auc_score, label))\n",
    "    plt.fill_between(recall, precision, alpha=0.5)\n",
    "    plt.grid(True)\n",
    "    plt.plot(recall, precision, lw=1)\n",
    "    filename = name.replace(\" \", \"_\")+'_pr'\n",
    "    save_png(filename)\n",
    "\n",
    "def plot_feat_importance(feature_names, clf, name):\n",
    "    plt.figure(num=None, figsize=(6, 5), dpi=300)\n",
    "    coef_ = clf.coef_\n",
    "    important = np.argsort(np.absolute(coef_.ravel()))\n",
    "    f_imp = feature_names[important]\n",
    "    coef = coef_.ravel()[important]\n",
    "    inds = np.argsort(coef)\n",
    "    f_imp = f_imp[inds]\n",
    "    coef = coef[inds]\n",
    "    xpos = np.array(list(range(len(coef))))\n",
    "    plt.bar(xpos, coef, width=1, alpha=0.75)\n",
    "\n",
    "    plt.title('Feature importance for %s' % (name.split('_')[-1]))\n",
    "    ax = plt.gca()\n",
    "    ax.set_xticks(np.arange(len(coef)))\n",
    "    labels = ax.set_xticklabels(f_imp)\n",
    "    for label in labels:\n",
    "        label.set_rotation(90)\n",
    "    filename = name.replace(\" \", \"_\")+'_feat_imp'\n",
    "    save_png(filename)\n",
    "\n",
    "def measure(clf_class, parameters, name, X, Y, data_size=None, plot=None, feature_names=None):\n",
    "    if data_size is not None:\n",
    "        X = X[:data_size]\n",
    "        Y = Y[:data_size]\n",
    "\n",
    "    train_errors = []\n",
    "    test_errors = []\n",
    "\n",
    "    scores = []\n",
    "    roc_scores = []\n",
    "    fprs, tprs = [], []\n",
    "\n",
    "    pr_scores = []\n",
    "    precisions, recalls, thresholds = [], [], []\n",
    "\n",
    "    for fold_idx, (train, test) in enumerate(cv.split(X, Y)):\n",
    "        X_train, y_train = X[train], Y[train]\n",
    "        X_test, y_test = X[test], Y[test]\n",
    "\n",
    "        only_one_class_in_train = len(set(y_train)) == 1\n",
    "        only_one_class_in_test = len(set(y_test)) == 1\n",
    "        if only_one_class_in_train or only_one_class_in_test:\n",
    "            # this would pose problems later on\n",
    "            continue\n",
    "\n",
    "        clf = clf_class(**parameters)\n",
    "\n",
    "        clf.fit(X_train, y_train)\n",
    "\n",
    "        train_score = clf.score(X_train, y_train)\n",
    "        test_score = clf.score(X_test, y_test)\n",
    "\n",
    "        train_errors.append(1 - train_score)\n",
    "        test_errors.append(1 - test_score)\n",
    "\n",
    "        scores.append(test_score)\n",
    "        proba = clf.predict_proba(X_test)\n",
    "\n",
    "        label_idx = 1\n",
    "        fpr, tpr, roc_thresholds = roc_curve(y_test, proba[:, label_idx])\n",
    "        precision, recall, pr_thresholds = precision_recall_curve(y_test, proba[:, label_idx])\n",
    "\n",
    "        roc_scores.append(auc(fpr, tpr))\n",
    "        fprs.append(fpr)\n",
    "        tprs.append(tpr)\n",
    "\n",
    "        pr_scores.append(auc(recall, precision))\n",
    "        precisions.append(precision)\n",
    "        recalls.append(recall)\n",
    "        thresholds.append(pr_thresholds)\n",
    "\n",
    "        # This threshold is determined at the end of the chapter,\n",
    "        # where we find conditions such that precision is in the area of\n",
    "        # about 80%. With it we trade off recall for precision.\n",
    "        threshold_for_detecting_good_answers = 0.59\n",
    "\n",
    "        if False:\n",
    "            print(\"Clone #%i\" % fold_idx)\n",
    "            print(classification_report(y_test, proba[:, label_idx] >\n",
    "                  threshold_for_detecting_good_answers, target_names=['not accepted', 'accepted']))\n",
    "\n",
    "    # get medium clone\n",
    "    scores_to_sort = pr_scores  # roc_scores\n",
    "    medium = np.argsort(scores_to_sort)[len(scores_to_sort) // 2]\n",
    "    # print(\"Medium clone is #%i\" % medium)\n",
    "\n",
    "    if plot:\n",
    "        #plot_roc(roc_scores[medium], name, fprs[medium], tprs[medium])\n",
    "        plot_pr(pr_scores[medium], name, precisions[medium],\n",
    "                recalls[medium], plot + \" answers\")\n",
    "\n",
    "        if hasattr(clf, 'coef_'):\n",
    "            plot_feat_importance(feature_names, clf, name)\n",
    "        elif hasattr(clf, 'named_steps'):\n",
    "            for step, s_clf in clf.named_steps.items():\n",
    "                if hasattr(s_clf, 'coef_'):\n",
    "                    plot_feat_importance(feature_names, s_clf, name)\n",
    "\n",
    "    summary = {'name': name,\n",
    "               'scores': scores,\n",
    "               'roc_scores': roc_scores,\n",
    "               'pr_scores': pr_scores,\n",
    "               'med_precisions': precisions[medium], \n",
    "               'med_recalls': recalls[medium], \n",
    "               'med_thresholds': thresholds[medium]}\n",
    "    \n",
    "    return np.mean(train_errors), np.mean(test_errors), summary\n",
    "\n",
    "def bias_variance_analysis(clf_class, parameters, name, X, Y):\n",
    "    data_sizes = np.arange(40, 2000, 20)\n",
    "\n",
    "    train_errors = []\n",
    "    test_errors = []\n",
    "\n",
    "    for data_size in data_sizes:\n",
    "        train_error, test_error, _ = measure(clf_class, parameters, name, X, Y, data_size=data_size)\n",
    "        train_errors.append(train_error)\n",
    "        test_errors.append(test_error)\n",
    "\n",
    "    plot_bias_variance(data_sizes, train_errors,\n",
    "                       test_errors, name, \"Bias-Variance for '%s'\" % name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we have learned in the previous chapter, when using features with different value ranges, it helps to standardize them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def create_pipeline(**param):\n",
    "    return make_pipeline(StandardScaler(), KNeighborsClassifier(**param))\n",
    "\n",
    "bias_variance_analysis(create_pipeline, {'n_neighbors': 5}, \"5NN\", X, Y)\n",
    "save_png('04_bv_5NN_all')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Maybe simplifying the feature space helps. Let's try out to use only `LinkCount` and `NumTextTokens`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import shuffle\n",
    "X_simp = np.asarray([get_features(aid, ['LinkCount', 'NumTextTokens']) for aid in all_answers], float)\n",
    "X_simp, Y_simp = shuffle(X_simp, Y, random_state=0)\n",
    "\n",
    "bias_variance_analysis(create_pipeline, {'n_neighbors': 5}, \"5NN\", X_simp, Y_simp)\n",
    "save_png('05_bv_5NN_simp')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Does it help to reduce the model complexity by increasing $k$?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('k\\tmean(scores)\\tstddev(scores)')\n",
    "for k in [5, 10, 40]:\n",
    "    _, _, summary = measure(create_pipeline, {'n_neighbors': k}, \"%iNN\" % k, X, Y)\n",
    "    print('%d\\t%.4f\\t\\t%.4f' % (k, np.mean(summary['scores']), np.std(summary['scores'])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It helps a bit, but do we really want to compare with 40 different samples each time?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_k_complexity(ks, train_errors, test_errors):\n",
    "    plt.figure(num=None, figsize=(6, 5), dpi=300)\n",
    "    plt.ylim([0.0, 1.0])\n",
    "    plt.xlabel('$k$')\n",
    "    plt.ylabel('Error')\n",
    "    plt.title('Errors for different values of $k$')\n",
    "    plt.plot(ks, test_errors, \"--\", ks, train_errors, \"-\", lw=1)\n",
    "    plt.legend([\"test error\", \"train error\"], loc=\"upper right\")\n",
    "    plt.grid(True)\n",
    "    save_png('06_kcomplexity')\n",
    "\n",
    "def k_complexity_analysis(clf_class, X, Y):\n",
    "    # Measure for different k's: [1,2,..,20,25,..,100]\n",
    "    ks = np.hstack((np.arange(1, 21), np.arange(25, 101, 5)))\n",
    "    \n",
    "    train_errors = []\n",
    "    test_errors = []\n",
    "\n",
    "    for k in ks:\n",
    "        train_error, test_error, _ = measure(clf_class, {'n_neighbors': k}, \"%dNN\" % k, X, Y, data_size=2000)\n",
    "        train_errors.append(train_error)\n",
    "        test_errors.append(test_error)\n",
    "\n",
    "    plot_k_complexity(ks, train_errors, test_errors)\n",
    "\n",
    "\n",
    "k_complexity_analysis(create_pipeline, X, Y)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we won't get much better with increasing values of $k$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using logistic regression\n",
    "Creating some toy data to visualize how logistic regression works..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import norm\n",
    "\n",
    "np.random.seed(3)\n",
    "\n",
    "NUM_PER_CLASS = 40\n",
    "X_log = np.hstack((norm.rvs(2, size=NUM_PER_CLASS, scale=2), norm.rvs(8, size=NUM_PER_CLASS, scale=3)))\n",
    "y_log = np.hstack((np.zeros(NUM_PER_CLASS), np.ones(NUM_PER_CLASS))).astype(int)\n",
    "\n",
    "plt.figure(figsize=(10, 4), dpi=300)\n",
    "plt.grid(True)\n",
    "\n",
    "plt.xlim((-5, 20))\n",
    "plt.scatter(X_log, y_log, c=np.array(['blue', 'red'])[y_log], s=10)\n",
    "plt.xlabel(\"Feature value\")\n",
    "plt.ylabel(\"Class\")\n",
    "save_png('06_log_reg_example_data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "def lr_model(clf, X):\n",
    "    '''\n",
    "    https://en.wikipedia.org/wiki/Logistic_regression\n",
    "    '''\n",
    "    return 1.0 / (1.0 + np.exp(-(clf.intercept_ + clf.coef_ * X)))\n",
    "\n",
    "logclf = LogisticRegression()\n",
    "logclf.fit(X_log.reshape(NUM_PER_CLASS * 2, 1), y_log)\n",
    "print(np.exp(logclf.intercept_), np.exp(logclf.coef_.ravel()))\n",
    "print(\"P(x=-1)=%.2f\\tP(x=7)=%.2f\" %(lr_model(logclf, -1), lr_model(logclf, 7)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "X_odds = np.arange(0.001, 1, 0.001)\n",
    "plt.figure(figsize=(10, 4), dpi=300)\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.xlim((0, 1))\n",
    "plt.ylim((0, 10))\n",
    "plt.plot(X_odds, X_odds / (1 - X_odds))\n",
    "plt.xlabel(\"P\")\n",
    "plt.ylabel(\"odds = P / (1-P)\")\n",
    "plt.grid(True)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.xlim((0, 1))\n",
    "plt.plot(X_odds, np.log(X_odds / (1 - X_odds)))\n",
    "plt.xlabel(\"P\")\n",
    "plt.ylabel(\"log(odds) = log(P / (1-P))\")\n",
    "plt.grid(True)\n",
    "save_png('07_log_reg_log_odds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "X_range = np.arange(-5, 20, 0.1)\n",
    "\n",
    "plt.figure(figsize=(10, 4), dpi=300)\n",
    "plt.xlim((-5, 20))\n",
    "plt.scatter(X_log, y_log, c=np.array(['blue', 'red'])[y_log], s=5)\n",
    "plt.plot(X_range, lr_model(logclf, X_range).ravel(), c='green')\n",
    "plt.plot(X_range, np.ones(X_range.shape[0]) * 0.5, \"--\")\n",
    "plt.xlabel(\"Feature value\")\n",
    "plt.ylabel(\"Class\")\n",
    "plt.grid(True)\n",
    "save_png('08_log_reg_example_fitted')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Applying logistic regression to our post classification problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print('C\\tmean(scores)\\tstddev(scores)') \n",
    "for C in [0.001, 0.01, 0.1, 1.0, 10.0]:\n",
    "    name = \"LogReg C=%.2f\" % C\n",
    "    _, _, summary = measure(LogisticRegression, {'C': C}, name, X, Y)\n",
    "\n",
    "    print('%7.3f\\t%.4f\\t\\t%.4f' % (C, np.mean(summary['scores']), np.std(summary['scores'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "C_best = 0.01"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Looking behind accuracy – precision and recall "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "bias_variance_analysis(LogisticRegression, {'C': C_best}, \"LogReg C=0.01\", X, Y)\n",
    "save_png('09_bv_LogReg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "feature_names = np.array((\n",
    "    'NumTextTokens',\n",
    "    'NumCodeLines',\n",
    "    'LinkCount',\n",
    "    'AvgSentLen',\n",
    "    'AvgWordLen',\n",
    "    'NumAllCaps',\n",
    "    'NumExclams'\n",
    "))\n",
    "\n",
    "X_orig = np.asarray([get_features(aid, ['LinkCount', 'NumCodeLines', 'NumTextTokens', 'AvgSentLen', 'AvgWordLen', 'NumAllCaps', 'NumExclams']) for aid in all_answers])\n",
    "\n",
    "Y_orig_good = np.asarray([meta[aid]['Score'] > 0 for aid in all_answers])\n",
    "Y_orig_poor = np.asarray([meta[aid]['Score'] <= 0 for aid in all_answers])\n",
    "\n",
    "X_new, Y_good, Y_poor = shuffle(X_orig, Y_orig_good, Y_orig_poor, random_state=0)\n",
    "    \n",
    "name = \"LogReg C=%.2f\" % C_best\n",
    "\n",
    "print(\"Good answers...\")\n",
    "_, _, good_results = measure(LogisticRegression, {'C': C_best}, '08_good_'+name, X_new, Y_good, plot='good', feature_names=feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(\"Poor answers...\")\n",
    "measure(LogisticRegression, {'C': C_best}, '09_poor_'+name, X_new, Y_poor, plot='poor', feature_names=feature_names);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precisions = good_results['med_precisions']\n",
    "recalls = good_results['med_recalls']\n",
    "thresholds = np.hstack([[0], good_results['med_thresholds']])\n",
    "\n",
    "for precision in np.arange(0.77, 0.8, 0.01):\n",
    "    thresh_idx = precisions >= precision\n",
    "    print(\"P=%.2f R=%.2f thresh=%.2f\" % (precisions[thresh_idx][0], recalls[thresh_idx][0], thresholds[thresh_idx][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "good_thresh = 0.66"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ship it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = LogisticRegression(C=C_best)\n",
    "clf.fit(X, Y)\n",
    "print(clf.coef_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "pickle.dump(clf, open(\"logreg.dat\", \"wb\"))\n",
    "clf = pickle.load(open(\"logreg.dat\", \"rb\"))\n",
    "print(clf.coef_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now use the classifier's `predict_proba()` to calculate the probabilities for the classes `poor` and `good`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remember that the features are in this order:\n",
    "# LinkCount, NumCodeLines, NumTextTokens, AvgSentLen, AvgWordLen, NumAllCaps, NumExclams\n",
    "good_post = (2, 1, 100, 5, 4, 1, 0)\n",
    "poor_post = (1, 0, 10, 5, 6, 5, 4)\n",
    "proba = clf.predict_proba([good_post, poor_post])\n",
    "proba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "proba >= good_thresh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, we manage to detect the first post as good, but cannot say anything about the second, which is why we would show some nice motivating message to improve the post."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification with Tensorflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also create a simple neural network with Tensorflow. As we already have the features, we can create a simple one with just one so-called hidden layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.asarray([get_features(aid, ['LinkCount', 'NumCodeLines', 'NumTextTokens', 'AvgSentLen', 'AvgWordLen', 'NumAllCaps', 'NumExclams']) for aid in all_answers])\n",
    "Y = np.asarray([meta[aid]['Score'] > 0 for aid in all_answers])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first covert the output reference. We want to target a probability as close as possible to 0 and 1, but as we use a sigmoid for the output layer that matches -inf to 0 and +inf to 1, we clamp instead to a small value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = Y.astype(np.float32)[:, None]\n",
    "bce_ceil = 1e-5\n",
    "Y = Y * (1 - 2 * bce_ceil) + bce_ceil"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We know split the data in train/test arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, train_size=0.8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create helper functions to create the layers. We don't need them now, but they are useful for playing with the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dense(x, n_units, name, alpha=0.2):\n",
    "    # Hidden layer\n",
    "    h = tf.layers.dense(x, n_units, activation=tf.nn.leaky_relu, name=name)\n",
    "    return h\n",
    "\n",
    "def create_output(x):\n",
    "    # Output layer\n",
    "    h = tf.layers.dense(x, 1, activation=tf.nn.sigmoid, name=\"Output\")\n",
    "    return h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "n_epochs = 500\n",
    "batch_size = 1000\n",
    "steps = 10\n",
    "layer1_size = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just as before, we can now create placeholders, and we will use our helper functions to create implicit variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tf = tf.placeholder(tf.float32, (None, 7), name=\"Input\")\n",
    "Y_ref_tf = tf.placeholder(tf.float32, (None, 1), name=\"Target_output\")\n",
    "\n",
    "h1 = create_dense(X_tf, layer1_size, name=\"Layer1\")\n",
    "Y_tf = create_output(h1)\n",
    "\n",
    "loss = tf.reduce_mean(tf.square(Y_ref_tf - Y_tf))\n",
    "\n",
    "grad_speed = .01\n",
    "my_opt = tf.train.GradientDescentOptimizer(grad_speed)\n",
    "train_step = my_opt.minimize(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now for the training session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    loss_vec = []\n",
    "    for epoch in range(n_epochs):\n",
    "        permut = np.random.permutation(len(X_train))\n",
    "        for j in range(0, len(X_train), batch_size):\n",
    "            batch = permut[j:j+batch_size]\n",
    "            Xs = X_train[batch]\n",
    "            Ys = Y_train[batch]\n",
    "            \n",
    "            sess.run(train_step, feed_dict={X_tf: Xs, Y_ref_tf: Ys})\n",
    "        \n",
    "        temp_loss = sess.run(loss, feed_dict={X_tf: X_train, Y_ref_tf: Y_train})\n",
    "        loss_vec.append(temp_loss)\n",
    "        if epoch % steps == steps - 1:\n",
    "            print('Epoch #%i  loss = %s' % (epoch, temp_loss))\n",
    "\n",
    "    predict_train = sess.run(Y_tf, feed_dict={X_tf: X_train})\n",
    "    predict_test = sess.run(Y_tf, feed_dict={X_tf: X_test})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(loss_vec, 'k-')\n",
    "plt.title('Loss per Epoch')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "save_png('tf_classification_loss')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now for the results for this new classifier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "score = accuracy_score(Y_train > .5, predict_train > .5)\n",
    "print(\"Score (on training data): %.2f\" % score)\n",
    "score = accuracy_score(Y_test > .5, predict_test > .5)\n",
    "print(\"Score (on testing data): %.2f\" % score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can very similar results to the other classifiers. It is now a good time to play with this neural network by changing the number of units inside the layer, adding a new layer, using different number of units for each layer, perhaps also changing the activation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before going to the next chapter, we can also display the confusion matrix thanks to a small function from scikit learn tutorials."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    import itertools\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    print(cm)\n",
    "\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = [\"Poor\", \"Good\"]\n",
    "from sklearn import metrics\n",
    "print(metrics.classification_report(Y_train > .5, predict_train > .5, target_names=class_names))\n",
    "plot_confusion_matrix(metrics.confusion_matrix(Y_train > .5, predict_train > .5), classes=class_names,\n",
    "                      title='Confusion matrix, without normalization')\n",
    "plt.grid(False)\n",
    "save_png('tf_classification_train')\n",
    "plt.show()\n",
    "print(metrics.classification_report(Y_test > .5, predict_test > .5, target_names=class_names))\n",
    "plot_confusion_matrix(metrics.confusion_matrix(Y_test > .5, predict_test > .5), classes=class_names,\n",
    "                      title='Confusion matrix, without normalization')\n",
    "plt.grid(False)\n",
    "save_png('tf_classification_test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
